{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d995fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "University of Liege\n",
    "ELEN0062 - Introduction to machine learning\n",
    "Project 1 - Classification algorithms\n",
    "\"\"\"\n",
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from plot import plot_boundary\n",
    "from data import make_balanced_dataset, make_unbalanced_dataset\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "def conditional_propabilty_of_positive_class(x, theta):\n",
    "    \"\"\"Computes conditional probability of sample x belonging to the positive\n",
    "        class knowing parameter theta and data sample X[i, :]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : vector-like, shape = [n_features]\n",
    "        The sample.\n",
    "\n",
    "    theta : vector-like, [omega_0, omega^T (vetor)]\n",
    "        Parameters of the sigmoÃ¯d.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : conditional probability of sample x belonging to the positive\n",
    "        class knowing parameter theta and data sample X[i, :]\n",
    "    \"\"\"\n",
    "\n",
    "    w_0 = theta[0] # real value\n",
    "    w = theta[1] # vector\n",
    "\n",
    "    p = 1/(1+exp(-w_0 - np.dot(np.transpose(w), x)))\n",
    "    return p\n",
    "\n",
    "def gradient_of_loss_function(X, theta):\n",
    "    sum = 0\n",
    "    N = np.shape(X)\n",
    "    for i in range(N):\n",
    "        x_prime = np.transpose(np.append(1, X[i]))\n",
    "        sum = sum + (conditional_propabilty_of_positive_class(X[i], )-y[i])*x_prime[i]\n",
    "    return (1/N)*sum\n",
    "\n",
    "def loss_function(theta, X):\n",
    "    sum = 0\n",
    "    N = np.shape(X)\n",
    "    for i in range(N):\n",
    "        sum = sum + log((conditional_propabilty_of_positive_class(X[i], theta)))\n",
    "    return -sum/N\n",
    "\n",
    "class LogisticRegressionClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_iter=10, learning_rate=1):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit a logistic regression models on (X, y)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        X = np.asarray(X, dtype=np.float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be 2 dimensional\")\n",
    "        n_instances, n_features = X.shape\n",
    "\n",
    "        y = np.asarray(y)\n",
    "        if y.shape[0] != X.shape[0]:\n",
    "            raise ValueError(\"The number of samples differs between X and y\")\n",
    "\n",
    "        n_classes = len(np.unique(y))\n",
    "        if n_classes != 2:\n",
    "            raise ValueError(\"This class is only dealing with binary \"\n",
    "                             \"classification problems\")\n",
    "\n",
    "\n",
    "        # TODO insert your code here\n",
    "\n",
    "        # Gradient descent to compute possible values of theta\n",
    "        theta = []\n",
    "        for i in range(self.n_iter):\n",
    "            theta_new = theta_old - self.learning_rate*gradient_loss(theta_old)\n",
    "            np.append(theta, theta_new)\n",
    "            theta_old = theta_new\n",
    "\n",
    "        # Now compute loss function for all values of theta\n",
    "        loss_functions = []\n",
    "        for theta_val in theta:\n",
    "            np.append(loss_functions, loss_function(theta_val, X))\n",
    "\n",
    "        # Find minimum loss function\n",
    "        min_index = argmin(loss_functions)\n",
    "        # Find corresponding theta value\n",
    "        optimal_theta = theta[min_index]\n",
    "\n",
    "        self.BaseEstimator.set_params(optimal_theta)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes, or the predict values.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO insert your code here\n",
    "        y = []\n",
    "        size = np.shape(X)\n",
    "        theta = self.BaseEstimator.get_params()\n",
    "        for i in range(size[0]):\n",
    "            if conditional_propabilty_of_positive_class(X[i], theta) >= 0.5:\n",
    "                y[i] = +1\n",
    "            else:\n",
    "                y[i] = -1\n",
    "\n",
    "        return y\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class probabilities of the input samples. Classes are ordered\n",
    "            by lexicographic order.\n",
    "        \"\"\"\n",
    "        # TODO insert your code here\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f94643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31f9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
