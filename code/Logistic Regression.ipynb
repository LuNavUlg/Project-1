{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2d995fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "University of Liege\n",
    "ELEN0062 - Introduction to machine learning\n",
    "Project 1 - Classification algorithms\n",
    "\"\"\"\n",
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from plot import plot_boundary\n",
    "from data import make_balanced_dataset, make_unbalanced_dataset\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "def conditional_propability_of_positive_class(x, omega0, omega):\n",
    "    \"\"\"Computes conditional probability of sample x belonging to the positive\n",
    "        class knowing parameter theta and data sample X[i, :]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : vector-like, shape = [n_features]\n",
    "        The sample.\n",
    "\n",
    "    theta : vector-like, [omega_0, omega^T (vetor)]\n",
    "        Parameters of the sigmoÃ¯d.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : conditional probability of sample x belonging to the positive\n",
    "        class knowing parameter theta and data sample X[i, :]\n",
    "    \"\"\"\n",
    "\n",
    "    p = 1/(1+math.exp(-omega0 - np.dot(np.transpose(omega), x)))\n",
    "    return p\n",
    "\n",
    "def gradient_of_loss_function(X, omega0, omega):\n",
    "    sum = 0\n",
    "    N = np.shape(X)\n",
    "    x_prime = []\n",
    "    for i in range(N[0]):\n",
    "        x_prime = np.append(x_prime, np.transpose(np.append(X[i], 1)))\n",
    "        sum = sum + np.dot((conditional_propability_of_positive_class(X[i], omega0, omega)-y[i]), x_prime[i])\n",
    "\n",
    "    return sum/N[0]\n",
    "\n",
    "def loss_function(X, omega0, omega):\n",
    "    sum = 0\n",
    "    N = np.shape(X)\n",
    "    for i in range(N):\n",
    "        sum = sum + np.log((conditional_propability_of_positive_class(X[i], omega0, omega)))\n",
    "    return -sum/N\n",
    "\n",
    "class LogisticRegressionClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_iter=10, learning_rate=1):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit a logistic regression models on (X, y)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        X = np.asarray(X, dtype=np.float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be 2 dimensional\")\n",
    "        n_instances, n_features = X.shape\n",
    "\n",
    "        y = np.asarray(y)\n",
    "        if y.shape[0] != X.shape[0]:\n",
    "            raise ValueError(\"The number of samples differs between X and y\")\n",
    "\n",
    "        n_classes = len(np.unique(y))\n",
    "        if n_classes != 2:\n",
    "            raise ValueError(\"This class is only dealing with binary \"\n",
    "                             \"classification problems\")\n",
    "\n",
    "\n",
    "        # TODO insert your code here\n",
    "\n",
    "        # Gradient descent to compute possible values of theta\n",
    "        omega0s = [] # vector of real values of omega0\n",
    "        omegas = [] # list of vectors\n",
    "        omega0_old  = 1\n",
    "        omega_old = (1, 1) \n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            omega0_new = omega0_old - self.learning_rate*gradient_of_loss_function(X, omega0_old, omega_old) # Computes new value w_0\n",
    "            omega_new = omega_old - self.learning_rate*gradient_of_loss_function(X, omega0_old, omega_old) # Computes new values w\n",
    "\n",
    "            omega0s.append(omega0_new)\n",
    "            omegas.append(omega_new)\n",
    "\n",
    "            omega0_old  = omega0_new\n",
    "            omega_old = omega_new\n",
    "\n",
    "        # Now compute loss function for all values of theta\n",
    "        loss_functions = []\n",
    "        for i in range(self.n_iter):\n",
    "            omega0 = omega0s[i]\n",
    "            omega = omegas[i]\n",
    "            value = gradient_of_loss_function(X, omega0, omega)\n",
    "            loss_functions.append(value)\n",
    "    \n",
    "        # Find minimum loss function\n",
    "        min_index = np.argmin(loss_functions)\n",
    "        # Find corresponding theta value\n",
    "        optimal_omega0 = omega0s[min_index]\n",
    "        optimal_omega = omegas[min_index]\n",
    "        \n",
    "        optimal_theta = [optimal_omega0, optimal_omega]\n",
    "\n",
    "        self.set_params(optimal_theta)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes, or the predict values.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO insert your code here\n",
    "        y = []\n",
    "        size = np.shape(X)\n",
    "        theta = self.BaseEstimator.get_params()\n",
    "        for i in range(size[0]):\n",
    "            if conditional_propabilty_of_positive_class(X[i], theta) >= 0.5:\n",
    "                y[i] = +1\n",
    "            else:\n",
    "                y[i] = -1\n",
    "\n",
    "        return y\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class probabilities of the input samples. Classes are ordered\n",
    "            by lexicographic order.\n",
    "        \"\"\"\n",
    "        # TODO insert your code here\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "02f94643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucie\\AppData\\Local\\Temp/ipykernel_17028/2363496698.py:82: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.asarray(X, dtype=np.float)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_params() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17028/2198924276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.33\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mlogistic_regression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mplot_boundary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"logisticR\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmesh_step_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Boundary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17028/2363496698.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0moptimal_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moptimal_omega0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimal_omega\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimal_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: set_params() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Put your code here\n",
    "    n_scores = []\n",
    "    X, y = make_unbalanced_dataset(3000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.33)\n",
    "\n",
    "    logistic_regression = LogisticRegressionClassifier().fit(X_train, y_train)\n",
    "    plot_boundary(\"logisticR\"+str(n), logistic_regression, X, y, mesh_step_size = 0.2, title=\"Boundary\")\n",
    "\n",
    "    #Compute mean score for corresponding n value and to n_scores vector\n",
    "    n_scores.append(np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e31f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [1, 2, 3, 4]\n",
    "theta_new = np.zeros(2)\n",
    "print(theta_new)\n",
    "\n",
    "np.dot(np.transpose(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fed12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d2a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
