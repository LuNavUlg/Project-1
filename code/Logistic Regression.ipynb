{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d995fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "University of Liege\n",
    "ELEN0062 - Introduction to machine learning\n",
    "Project 1 - Classification algorithms\n",
    "\"\"\"\n",
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from plot import plot_boundary\n",
    "from data import make_balanced_dataset, make_unbalanced_dataset\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "def conditional_probability_of_positive_class(x, omega0, omega):\n",
    "    \"\"\"Computes conditional probability of sample x belonging to the positive\n",
    "        class knowing parameter theta and data sample X[i, :]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : vector-like, shape = [n_features]\n",
    "        The sample.\n",
    "\n",
    "    omega0 and omega :\n",
    "        Parameters of the sigmoÃ¯d.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : conditional probability of sample x belonging to the positive\n",
    "        class knowing parameter theta and data sample X[i, :]\n",
    "    \"\"\"\n",
    "\n",
    "    p = 1/(1+math.exp(-omega0 - np.dot(np.transpose(omega), x)))\n",
    "    return p\n",
    "\n",
    "def gradient_of_loss_function(X, omega0, omega):\n",
    "    sum = 0\n",
    "    N = np.shape(X)\n",
    "    x_prime = []\n",
    "    for i in range(N[0]):\n",
    "        x_prime = np.append(x_prime, np.transpose(np.append(X[i], 1)))\n",
    "        sum = sum + np.dot((conditional_probability_of_positive_class(X[i], omega0, omega)-y[i]), x_prime[i])\n",
    "\n",
    "    return sum/N[0]\n",
    "\n",
    "def loss_function(X, omega0, omega):\n",
    "    sum = 0\n",
    "    N = np.shape(X)\n",
    "    for i in range(N[0]):\n",
    "        sum = sum + np.log((conditional_probability_of_positive_class(X[i], omega0, omega)))\n",
    "    return -sum/N[0]\n",
    "\n",
    "class LogisticRegressionClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_iter=10, learning_rate=1):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.omega0 = None\n",
    "        self.omega = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit a logistic regression models on (X, y)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        X = np.asarray(X, dtype=np.float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be 2 dimensional\")\n",
    "        n_instances, n_features = X.shape\n",
    "\n",
    "        y = np.asarray(y)\n",
    "        if y.shape[0] != X.shape[0]:\n",
    "            raise ValueError(\"The number of samples differs between X and y\")\n",
    "\n",
    "        n_classes = len(np.unique(y))\n",
    "        if n_classes != 2:\n",
    "            raise ValueError(\"This class is only dealing with binary \"\n",
    "                             \"classification problems\")\n",
    "\n",
    "\n",
    "        # TODO insert your code here\n",
    "\n",
    "        # Gradient descent to compute possible values of theta\n",
    "        omega0s = [] # vector of real values of omega0\n",
    "        omegas = [] # list of vectors\n",
    "        omega0_old  = 1\n",
    "        omega_old = (1, 1) \n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            omega0_new = omega0_old - self.learning_rate*gradient_of_loss_function(X, omega0_old, omega_old) # Computes new value w_0\n",
    "            omega_new = omega_old - self.learning_rate*gradient_of_loss_function(X, omega0_old, omega_old) # Computes new values w\n",
    "\n",
    "            omega0s.append(omega0_new)\n",
    "            omegas.append(omega_new)\n",
    "\n",
    "            omega0_old  = omega0_new\n",
    "            omega_old = omega_new\n",
    "\n",
    "        # Now compute loss function for all values of theta\n",
    "        loss_functions = []\n",
    "        for i in range(self.n_iter):\n",
    "            omega0 = omega0s[i]\n",
    "            omega = omegas[i]\n",
    "            value = loss_function(X, omega0, omega)\n",
    "            loss_functions.append(value)\n",
    "    \n",
    "        # Find minimum loss function\n",
    "        min_index = np.argmin(loss_functions)\n",
    "        # Find corresponding theta value\n",
    "        optimal_omega0 = omega0s[min_index]\n",
    "        optimal_omega = omegas[min_index]\n",
    "        \n",
    "        self.omega0 = optimal_omega0\n",
    "        self.omega = optimal_omega\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes, or the predict values.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO insert your code here\n",
    "        y = []\n",
    "        N = np.shape(X)\n",
    "        proba = self.predict_proba(X)\n",
    "        for i in range(N[0]):\n",
    "            if proba[i, 0] >= 0.5:\n",
    "                y.append(+1)\n",
    "            else:\n",
    "                y.append(-1)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class probabilities of the input samples. Classes are ordered\n",
    "            by lexicographic order.\n",
    "        \"\"\"\n",
    "        # TODO insert your code here\n",
    "\n",
    "        proba = []\n",
    "        N = np.shape(X)\n",
    "        omega0 = self.omega0\n",
    "        omega = self.omega\n",
    "\n",
    "        # COL 0 : POSITIVE CLASS -- COL 1 : NEGATIVE CLASS \n",
    "        # Warning this has to be coherent with variable proba of predict() method !\n",
    "        for i in range(N[0]):\n",
    "            p = conditional_probability_of_positive_class(X[i], omega0, omega)\n",
    "            row = [p, 1-p]\n",
    "            proba.append(row)\n",
    "            \n",
    "        proba = np.array(proba)\n",
    "        return proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02f94643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucie\\AppData\\Local\\Temp/ipykernel_2596/3011207567.py:83: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.asarray(X, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Put your code here\n",
    "    X, y = make_unbalanced_dataset(3000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.33)\n",
    "\n",
    "    logistic_regression = LogisticRegressionClassifier().fit(X_train, y_train)\n",
    "    \n",
    "    plot_boundary(\"logistic_regression\", logistic_regression, X, y, mesh_step_size=0.1, title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31f9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fed12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d2a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
