{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d995fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "University of Liege\n",
    "ELEN0062 - Introduction to machine learning\n",
    "Project 1 - Classification algorithms\n",
    "\"\"\"\n",
    "#! /usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from plot import plot_boundary\n",
    "from data import make_balanced_dataset, make_unbalanced_dataset\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def conditional_probability_of_positive_class(x, omega0, omega):\n",
    "    \"\"\"Computes conditional probability of sample x belonging to the positive\n",
    "        class knowing parameter theta and data sample X[i, :]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : vector-like, shape = [n_features]\n",
    "        The sample.\n",
    "\n",
    "    omega0 and omega :\n",
    "        Parameters of the sigmoÃ¯d.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : conditional probability of sample x belonging to the positive\n",
    "        class knowing parameter theta and data sample X[i, :]\n",
    "    \"\"\"\n",
    "\n",
    "    p = 1/(1+math.exp(-omega0 - omega.dot(x)))\n",
    "    return p\n",
    "\n",
    "def gradient_of_loss_function(X, omega0, omega):\n",
    "    omega_sum = np.array([0.0, 0.0])\n",
    "    omega0_sum = 0.0\n",
    "    N = np.shape(X)[0]\n",
    "    x_prime = []\n",
    "    for i in range(N):\n",
    "        factor = conditional_probability_of_positive_class(X[i], omega0, omega)-y[i]\n",
    "        omega_sum +=  factor * X[i]\n",
    "        omega0_sum += factor\n",
    "\n",
    "    return omega_sum/N, omega0_sum/N\n",
    "\n",
    "class LogisticRegressionClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_iter=10, learning_rate=1):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.omega0 = None\n",
    "        self.omega = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit a logistic regression models on (X, y)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Input validation\n",
    "        X = np.asarray(X, dtype=np.float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X must be 2 dimensional\")\n",
    "        n_instances, n_features = X.shape\n",
    "\n",
    "        y = np.asarray(y)\n",
    "        if y.shape[0] != X.shape[0]:\n",
    "            raise ValueError(\"The number of samples differs between X and y\")\n",
    "\n",
    "        n_classes = len(np.unique(y))\n",
    "        if n_classes != 2:\n",
    "            raise ValueError(\"This class is only dealing with binary \"\n",
    "                             \"classification problems\")\n",
    "\n",
    "\n",
    "        # TODO insert your code here\n",
    "\n",
    "        # Gradient descent to compute possible values of theta\n",
    "        omega0  = 1.0\n",
    "        omega = np.array((1.0, 1.0)) \n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            loss_omega, loss_omega0 = gradient_of_loss_function(X, omega0, omega)\n",
    "            omega0 = omega0 - self.learning_rate*loss_omega0 # Computes new value w_0\n",
    "            omega = omega - self.learning_rate*loss_omega # Computes new value w\n",
    "        \n",
    "        # When iterations of gradient descent are done, omega0 and omega are supposed optimal and must be stored\n",
    "        self.omega0 = omega0\n",
    "        self.omega = omega\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes, or the predict values.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO insert your code here\n",
    "        y = []\n",
    "        N = np.shape(X)\n",
    "        proba = self.predict_proba(X)\n",
    "        for i in range(N[0]):\n",
    "            if proba[i, 1] >= 0.5:\n",
    "                y.append(+1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class probabilities of the input samples. Classes are ordered\n",
    "            by lexicographic order.\n",
    "        \"\"\"\n",
    "        # TODO insert your code here\n",
    "\n",
    "        proba = []\n",
    "        N = np.shape(X)\n",
    "        omega0 = self.omega0\n",
    "        omega = self.omega\n",
    "\n",
    "        # COL 0 : NEGATIVE CLASS -- COL 1 : POSITIVE CLASS \n",
    "        # Warning this has to be coherent with variable proba of predict() method !\n",
    "        for i in range(N[0]):\n",
    "            p = conditional_probability_of_positive_class(X[i], omega0, omega)\n",
    "            row = [1-p, p]\n",
    "            proba.append(row)\n",
    "            \n",
    "        proba = np.array(proba)\n",
    "        return proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f94643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucie\\AppData\\Local\\Temp/ipykernel_20152/1998328707.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.asarray(X, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Put your code here\n",
    "    X, y = make_unbalanced_dataset(3000)\n",
    "\n",
    "    logistic_regression = LogisticRegressionClassifier().fit(X[:1000], y[:1000])\n",
    "    \n",
    "    plot_boundary(\"logistic_regression\", logistic_regression, X, y, mesh_step_size=0.1, title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e31f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucie\\AppData\\Local\\Temp/ipykernel_20152/1998328707.py:78: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.asarray(X, dtype=np.float)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20152/2396119614.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlogistic_regression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean accuracy : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "    # Here we will report the avg accuracy over five generations of the dataset along with its standard deviation\n",
    "    gen = 5\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(gen):\n",
    "        X, y = make_unbalanced_dataset(3000)\n",
    "\n",
    "        logistic_regression = LogisticRegressionClassifier()\n",
    "        logistic_regression.fit(X[:1000], y[:1000])\n",
    "        accuracies.append(accuracy_score(y[1000:], logistic_regression.predict(X)))\n",
    "    \n",
    "    print(\"Mean accuracy : \", np.mean(accuracies))\n",
    "    print(\"Std : \", np.std(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8fed12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucie\\AppData\\Local\\Temp/ipykernel_2108/3011207567.py:83: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.asarray(X, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy LR1 :  0.04378109452736319\n",
      "Mean accuracy LR2 :  0.04378109452736319\n",
      "Mean accuracy LR3 :  0.04378109452736319\n",
      "Mean accuracy LR4 :  0.04378109452736319\n"
     ]
    }
   ],
   "source": [
    "    # Here we'll study the effects of the number of iterations on the decision boundary and the error\n",
    "    accuracies_LR1 = []\n",
    "    accuracies_LR2 = []\n",
    "    accuracies_LR3 = []\n",
    "    accuracies_LR4 = []\n",
    "    \n",
    "    X, y = make_unbalanced_dataset(3000)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.33)\n",
    "\n",
    "    LR1 = LogisticRegressionClassifier(n_iter=3, learning_rate=0.6)\n",
    "    LR2 = LogisticRegressionClassifier(learning_rate = 0.6) # default n_iter=10, learning_rate=1\n",
    "    LR3 = LogisticRegressionClassifier(n_iter=20, learning_rate=0.6)\n",
    "    LR4 = LogisticRegressionClassifier(n_iter=100, learning_rate=0.6)\n",
    "    \n",
    "    LR1.fit(X_train, y_train)\n",
    "    LR2.fit(X_train, y_train)\n",
    "    LR3.fit(X_train, y_train)\n",
    "    LR4.fit(X_train, y_train)\n",
    "    \n",
    "    plot_boundary(\"LR1\", LR1, X, y, mesh_step_size=0.1, title=\"Boundary for LR1\")\n",
    "    plot_boundary(\"LR2\", LR2, X, y, mesh_step_size=0.1, title=\"Boundary for LR2\")\n",
    "    plot_boundary(\"LR3\", LR3, X, y, mesh_step_size=0.1, title=\"Boundary for LR3\")\n",
    "    plot_boundary(\"LR4\", LR4, X, y, mesh_step_size=0.1, title=\"Boundary for LR4\")\n",
    "    \n",
    "    accuracies_LR1.append(accuracy_score(y_test, LR1.predict(X_test)))\n",
    "    accuracies_LR2.append(accuracy_score(y_test, LR2.predict(X_test)))\n",
    "    accuracies_LR3.append(accuracy_score(y_test, LR3.predict(X_test)))\n",
    "    accuracies_LR4.append(accuracy_score(y_test, LR4.predict(X_test)))\n",
    "    \n",
    "    print(\"Mean accuracy LR1 : \", np.mean(accuracies_LR1))\n",
    "    print(\"Mean accuracy LR2 : \", np.mean(accuracies_LR2))\n",
    "    print(\"Mean accuracy LR3 : \", np.mean(accuracies_LR3))\n",
    "    print(\"Mean accuracy LR4 : \", np.mean(accuracies_LR4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d2a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
